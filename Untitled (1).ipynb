{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "834bebe6-101f-4362-a8c3-493f889dbe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040305ed-5c61-40e1-8c9c-51a87b154ed5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 0. Convert all image datas to jpeg format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8348bea-f034-4f70-a277-73cb00c9ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def convert_images_to_jpeg(input_directory, output_directory, target_size=(256*2, 256*2)):\n",
    "    \"\"\"Resizes all images in a directory to the target size and saves them in another directory.\n",
    "\n",
    "    Args:\n",
    "        input_directory: The path to the directory containing the images to resize.\n",
    "        output_directory: The path to the directory to save the resized images.\n",
    "        target_size: The desired target size of the images.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\".png\"):\n",
    "            input_image_path = os.path.join(input_directory, file)\n",
    "            output_image_path = os.path.join(output_directory, file)\n",
    "            \n",
    "            image = Image.open(input_image_path)\n",
    "            resized_image = image.resize(target_size, Image.LANCZOS)\n",
    "\n",
    "            if resized_image.mode != 'RGB':\n",
    "                resized_image = resized_image.convert('RGB')\n",
    "\n",
    "            resized_image.save(output_image_path, format='JPEG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33110358-5ed4-4ef5-aa6b-7f8a6d5f686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_images_to_jpeg(\"mountain_dataset\", \"resized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2537955f-51d9-433d-a6d7-7fef48a888b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"{device} is set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f16d64-b053-4f4c-9691-12818a888917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path, transform):\n",
    "    train_dataset = torchvision.datasets.DatasetFolder(\n",
    "        root=data_path,\n",
    "        loader=torchvision.datasets.folder.default_loader,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return train_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746316d7-ea87-466b-83c9-38d5b621ad5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model trained on 4K usual resolution images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cd3071-6896-4154-a28e-bacb400057ee",
   "metadata": {},
   "source": [
    "## 1. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f79364-2c19-448e-8a44-50fc0e4831e0",
   "metadata": {},
   "source": [
    "### 1.0 Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716c8317-2a97-4e06-b70b-26bd1942a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_dim):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(img_dim, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e549d-da24-4970-a565-cbcda03668de",
   "metadata": {},
   "source": [
    "### 1.1 Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78796fe5-29ae-4531-8c43-532fcf0e22ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(1024, img_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d04658-9c68-4a5f-8b65-e9c80a46f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001 #1e-4\n",
    "z_dim = 64\n",
    "image_dim = 256 * 256 * 3\n",
    "batch_size = 32\n",
    "num_epochs = 80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e96413-08c5-4ff9-8ae3-1e0e40a186a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd06262-414a-4391-8a06-2f9e48a03642",
   "metadata": {},
   "source": [
    "### 2. Load Custom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3086a8-e6dd-4315-873f-8d05cb06069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(data_path) if f.endswith('.jpg') or f.endswith('.jpeg') or f.endswith('.png')]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.data_path, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "data_path = \"resized_dataset\"\n",
    "dataset = CustomDataset(data_path, transform=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6513e046-a21d-43d1-885a-6e7a28ca53fb",
   "metadata": {},
   "source": [
    "### 2.1 Generate loader, Discriminator optimizer, Generator optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0ed16-4424-46e8-a728-5284fe78c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cf6c2e-dc47-462e-94ac-855e4f06fb21",
   "metadata": {},
   "source": [
    "## 3.Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e3d2c-def7-44b3-ab31-d1cb80b50b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, real in enumerate(loader):\n",
    "        real = real.view(-1, image_dim).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ### Train Discriminator: max log(D(real)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator maximize log(D(G(z)))\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch: [{epoch+1}/{num_epochs}], Loss D: {lossD:.4f}, Loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                index = random.randint(0, batch_size-1)\n",
    "                fake = gen(fixed_noise[index]).reshape(3, 256, 256)\n",
    "                save_image(fake, f\"generated_images/epoch{epoch}.png\", normalize=True)\n",
    "                step += 1\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3546c-d7fa-4921-a0b4-74a878ebeaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Discriminator Parameters:\\t{sum(p.numel() for p in disc.parameters())}\\n\"\n",
    "    f\"Generator Parameters:\\t\\t{sum(p.numel() for p in gen.parameters())}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2680cc1-8cd5-4acf-bca8-3350f54a61bf",
   "metadata": {},
   "source": [
    "## 4.Generate example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d3a54-1f19-4057-a831-5bd00c97de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming you have 'gen' (generator) already defined and trained\n",
    "generator = gen\n",
    "\n",
    "# Set the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Generate images using random noise\n",
    "num_images = 10  # Number of images to generate\n",
    "z_dim = 64      # Size of the generator's input noise vector\n",
    "\n",
    "# Generate random noise\n",
    "noise = torch.randn(num_images, z_dim).to(device)\n",
    "\n",
    "# Generate fake images\n",
    "fake_images = generator(noise)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = 'generated_images'\n",
    "\n",
    "# Save generated images\n",
    "from torchvision import utils\n",
    "\n",
    "def convert_tensor_to_image(tensor):\n",
    "    \"\"\"Converts a torch tensor to a PIL Image.\"\"\"\n",
    "    image = transforms.ToPILImage()(tensor.reshape(3, 256, 256))\n",
    "    return image\n",
    "\n",
    "# Convert the fake images to PIL Images\n",
    "image_names = [f'generated_image_{i+1}.png' for i in range(num_images)]\n",
    "\n",
    "pil_images = [convert_tensor_to_image(image) for image in fake_images]\n",
    "\n",
    "# Apply the custom transformations\n",
    "preprocessed_images = [transform(image) for image in pil_images]\n",
    "\n",
    "# Save the preprocessed images\n",
    "for image, name in zip(preprocessed_images, image_names):\n",
    "    utils.save_image(image, os.path.join(output_dir, name))\n",
    "\n",
    "print(f'{num_images} images generated and saved in {output_dir}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc64176-d335-47cb-8f51-5a3cc1329268",
   "metadata": {},
   "source": [
    "## 5.Save & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b1dfad-405d-45c6-b9ff-e81d69dac43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'saved_model/'\n",
    "torch.save(gen.state_dict(), os.path.join(model_dir, 'final_generator.pth'))\n",
    "torch.save(disc.state_dict(), os.path.join(model_dir, 'final_discriminator.pth'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32ef0b17-e78e-4490-a54e-322a3aeb7d20",
   "metadata": {},
   "source": [
    "# Create instances of the model classes\n",
    "loaded_gen = Generator(z_dim, image_dim).to(device)\n",
    "loaded_disc = Discriminator(image_dim).to(device)\n",
    "\n",
    "# Load saved state dictionaries\n",
    "gen_checkpoint = torch.load('saved_model/final_generator.pth')\n",
    "disc_checkpoint = torch.load('saved_model/final_discriminator.pth')\n",
    "\n",
    "loaded_gen.load_state_dict(gen_checkpoint)\n",
    "loaded_disc.load_state_dict(disc_checkpoint)\n",
    "\n",
    "# Set the models to evaluation mode\n",
    "loaded_gen.eval()\n",
    "loaded_disc.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b29a8-d332-4ed4-8a8f-75d067a6f033",
   "metadata": {},
   "source": [
    "# Model trained on 13GB of high resolution images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d1e9b-3dfb-47ef-8d6f-ce41c38f80f7",
   "metadata": {},
   "source": [
    "## More complex Discriminator and Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11585ea2",
   "metadata": {},
   "source": [
    "* Training on 768x768 high resolution images\n",
    "* More complex Discriminator\n",
    "* Longer training\n",
    "* z_dim = 100\n",
    "* Different Loss Function, batch size, learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aefabae3-3146-48b5-8c6c-7dc238f04804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is set\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"{device} is set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5d7312d9-5db4-437f-bd44-ae2a1f9dc987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_dim):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(img_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 512),    # Input: z_dim, Output: 1024\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),    # Input: 1024, Output: 2048\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 2048),    # Input: 2048, Output: 4096\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(2048, 1024+512),    # Input: 4096, Output: 8192\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024+512, 1024+512+256),   # Input: 8192, Output: 16384\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024+512+256, 1024),  # Input: 16384, Output: 32768\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 512),  # Input: 16384, Output: 32768\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, img_dim), # Input: 32768, Output: img_dim\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59572e1e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def convert_images_to_jpeg(input_directory, output_directory, target_size=(512, 512)):\n",
    "    \"\"\"Resizes all images in a directory to the target size and saves them in another directory.\n",
    "\n",
    "    Args:\n",
    "        input_directory: The path to the directory containing the images to resize.\n",
    "        output_directory: The path to the directory to save the resized images.\n",
    "        target_size: The desired target size of the images.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\".png\"):\n",
    "            input_image_path = os.path.join(input_directory, file)\n",
    "            output_image_path = os.path.join(output_directory, file)\n",
    "            \n",
    "            image = Image.open(input_image_path)\n",
    "            resized_image = image.resize(target_size, Image.LANCZOS)\n",
    "\n",
    "            # if resized_image.mode != 'RGB':\n",
    "            #     resized_image = resized_image.convert('RGB')\n",
    "\n",
    "            resized_image.save(output_image_path, format='JPEG')\n",
    "convert_images_to_jpeg(\"mountain_dataset\\dataset\", \"resized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c087693-3aee-48ad-bc94-79bd1bf8b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00002\n",
    "z_dim = 100\n",
    "image_dim = 256 * 256 * 3\n",
    "batch_size = 32\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2f37d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "86baaeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(data_path) if f.endswith('.jpg') or f.endswith('.jpeg') or f.endswith('.png')]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.data_path, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "data_path = \"resized_dataset\"\n",
    "dataset = CustomDataset(data_path, transform=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8de2b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "502d8c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=64, shuffle=True, pin_memory=True)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "61738534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator Parameters:\t101484674\n",
      "Generator Parameters:\t\t111798528\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Discriminator Parameters:\\t{sum(p.numel() for p in disc.parameters())}\\n\"\n",
    "    f\"Generator Parameters:\\t\\t{sum(p.numel() for p in gen.parameters())}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "03eee006-f864-4c6c-adce-aea273853928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    \n",
    "    # Print the available GPU devices\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPUs available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e352c56-05ac-46dd-8e74-440bf06f4b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100], Loss D: 0.6938, Loss G: 0.7250\n",
      "Epoch: [2/100], Loss D: 0.0884, Loss G: 1.9636\n",
      "Epoch: [3/100], Loss D: 0.0241, Loss G: 3.6243\n",
      "Epoch: [4/100], Loss D: 0.0000, Loss G: 100.0000\n",
      "Epoch: [5/100], Loss D: 0.0000, Loss G: 100.0000\n",
      "Epoch: [6/100], Loss D: 0.0000, Loss G: 100.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, real \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[0;32m     11\u001b[0m         real \u001b[38;5;241m=\u001b[39m real\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, image_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m real\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\OneDrive\\Masaüstü\\yolo\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\OneDrive\\Masaüstü\\yolo\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\OneDrive\\Masaüstü\\yolo\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\OneDrive\\Masaüstü\\yolo\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[66], line 13\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     12\u001b[0m     img_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_files[idx])\n\u001b[1;32m---> 13\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     15\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[1;32m~\\OneDrive\\Masaüstü\\yolo\\venv\\Lib\\site-packages\\PIL\\Image.py:921\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    875\u001b[0m ):\n\u001b[0;32m    876\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    923\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Masaüstü\\yolo\\venv\\Lib\\site-packages\\PIL\\ImageFile.py:242\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 242\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecodermaxblock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, struct\u001b[38;5;241m.\u001b[39merror) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;66;03m# truncated png/gif\u001b[39;00m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "File \u001b[1;32m~\\OneDrive\\Masaüstü\\yolo\\venv\\Lib\\site-packages\\PIL\\JpegImagePlugin.py:403\u001b[0m, in \u001b[0;36mJpegImageFile.load_read\u001b[1;34m(self, read_bytes)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, read_bytes):\n\u001b[0;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m    internal: read more image data\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    For premature EOF and LOAD_TRUNCATED_IMAGES adds EOI marker\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    so libjpeg can finish decoding\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(read_bytes)\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m ImageFile\u001b[38;5;241m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ended\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;66;03m# Premature EOF.\u001b[39;00m\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;66;03m# Pretend file is finished adding EOI marker\u001b[39;00m\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, real in enumerate(loader):\n",
    "        real = real.view(-1, image_dim).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ### Train Discriminator: max log(D(real)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator maximize log(D(G(z)))\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch: [{epoch+1}/{num_epochs}], Loss D: {lossD:.4f}, Loss G: {lossG:.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4a49886f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 images generated and saved in generated_images\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming you have 'gen' (generator) already defined and trained\n",
    "generator = gen\n",
    "\n",
    "# Generate images using random noise\n",
    "num_images = 10  # Number of images to generate\n",
    "z_dim = 100      # Size of the generator's input noise vector\n",
    "\n",
    "# Generate random noise\n",
    "noise = torch.randn(num_images, z_dim).to(device)\n",
    "\n",
    "# Generate fake images\n",
    "fake_images = generator(noise)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = 'generated_images'\n",
    "\n",
    "# Save generated images\n",
    "from torchvision import utils\n",
    "\n",
    "def convert_tensor_to_image(tensor):\n",
    "    \"\"\"Converts a torch tensor to a PIL Image.\"\"\"\n",
    "    image = transforms.ToPILImage()(tensor.reshape(3, 256, 256))\n",
    "    return image\n",
    "\n",
    "# Convert the fake images to PIL Images\n",
    "image_names = [f'generated_image_{i+1}.png' for i in range(num_images)]\n",
    "\n",
    "pil_images = [convert_tensor_to_image(image) for image in fake_images]\n",
    "\n",
    "# Apply the custom transformations\n",
    "preprocessed_images = [transform(image) for image in pil_images]\n",
    "\n",
    "# Save the preprocessed images\n",
    "for image, name in zip(preprocessed_images, image_names):\n",
    "    utils.save_image(image, os.path.join(output_dir, name))\n",
    "\n",
    "print(f'{num_images} images generated and saved in {output_dir}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4b5631c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'saved_model/'\n",
    "os.makedirs(model_dir)\n",
    "torch.save(gen.state_dict(), os.path.join(model_dir, 'final_generator_90k.pth'))\n",
    "torch.save(disc.state_dict(), os.path.join(model_dir, 'final_discriminator_90k.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3a4f16-4dfc-4e8e-8862-04f72f42057b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
