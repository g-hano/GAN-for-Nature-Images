{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "040305ed-5c61-40e1-8c9c-51a87b154ed5",
   "metadata": {},
   "source": [
    "## 0. Convert all image datas to jpeg format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8348bea-f034-4f70-a277-73cb00c9ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def convert_images_to_jpeg(input_directory, output_directory, target_size=(256*2, 256*2)):\n",
    "    \"\"\"Resizes all images in a directory to the target size and saves them in another directory.\n",
    "\n",
    "    Args:\n",
    "        input_directory: The path to the directory containing the images to resize.\n",
    "        output_directory: The path to the directory to save the resized images.\n",
    "        target_size: The desired target size of the images.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\".png\"):\n",
    "            input_image_path = os.path.join(input_directory, file)\n",
    "            output_image_path = os.path.join(output_directory, file)\n",
    "            \n",
    "            image = Image.open(input_image_path)\n",
    "            resized_image = image.resize(target_size, Image.LANCZOS)\n",
    "\n",
    "            if resized_image.mode != 'RGB':\n",
    "                resized_image = resized_image.convert('RGB')\n",
    "\n",
    "            resized_image.save(output_image_path, format='JPEG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33110358-5ed4-4ef5-aa6b-7f8a6d5f686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_images_to_jpeg(\"mountain_dataset\", \"resized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "834bebe6-101f-4362-a8c3-493f889dbe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2537955f-51d9-433d-a6d7-7fef48a888b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is set\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"{device} is set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78f16d64-b053-4f4c-9691-12818a888917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path, transform):\n",
    "    train_dataset = torchvision.datasets.DatasetFolder(\n",
    "        root=data_path,\n",
    "        loader=torchvision.datasets.folder.default_loader,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return train_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cd3071-6896-4154-a28e-bacb400057ee",
   "metadata": {},
   "source": [
    "## 1. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f79364-2c19-448e-8a44-50fc0e4831e0",
   "metadata": {},
   "source": [
    "### 1.0 Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "716c8317-2a97-4e06-b70b-26bd1942a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_dim):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(img_dim, 128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e549d-da24-4970-a565-cbcda03668de",
   "metadata": {},
   "source": [
    "### 1.1 Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78796fe5-29ae-4531-8c43-532fcf0e22ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(1024, img_dim),\n",
    "            nn.Tanh() # bunu kullanma sebebi sonuçları -1, 1 arasında almak istemesi, ona göre fake real diye kıyaslıcak\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69d04658-9c68-4a5f-8b65-e9c80a46f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01 #1e-4\n",
    "z_dim = 64\n",
    "image_dim = 256 * 256 * 3\n",
    "batch_size = 32\n",
    "num_epochs = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5e96413-08c5-4ff9-8ae3-1e0e40a186a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea68a5-7fac-4c89-8409-419ac82cc5f4",
   "metadata": {},
   "source": [
    "#### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdddd461-5743-4292-b376-3613923d8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd06262-414a-4391-8a06-2f9e48a03642",
   "metadata": {},
   "source": [
    "### 2. Load Custom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be3086a8-e6dd-4315-873f-8d05cb06069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(data_path) if f.endswith('.jpg') or f.endswith('.jpeg') or f.endswith('.png')]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.data_path, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "data_path = \"resized_dataset\"\n",
    "dataset = CustomDataset(data_path, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6513e046-a21d-43d1-885a-6e7a28ca53fb",
   "metadata": {},
   "source": [
    "### 2.1 Generate loader, Discriminator optimizer, Generator optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ab0ed16-4424-46e8-a728-5284fe78c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=0, shuffle=True)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cf6c2e-dc47-462e-94ac-855e4f06fb21",
   "metadata": {},
   "source": [
    "## 3.Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c51e3d2c-def7-44b3-ab31-d1cb80b50b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30]\n",
      "Epoch: [2/30]\n",
      "Epoch: [3/30]\n",
      "Epoch: [4/30]\n",
      "Epoch: [5/30]\n",
      "Epoch: [6/30]\n",
      "Epoch: [7/30]\n",
      "Epoch: [8/30]\n",
      "Epoch: [9/30]\n",
      "Epoch: [10/30]\n",
      "Epoch: [11/30]\n",
      "Epoch: [12/30]\n",
      "Epoch: [13/30]\n",
      "Epoch: [14/30]\n",
      "Epoch: [15/30]\n",
      "Epoch: [16/30]\n",
      "Epoch: [17/30]\n",
      "Epoch: [18/30]\n",
      "Epoch: [19/30]\n",
      "Epoch: [20/30]\n",
      "Epoch: [21/30]\n",
      "Epoch: [22/30]\n",
      "Epoch: [23/30]\n",
      "Epoch: [24/30]\n",
      "Epoch: [25/30]\n",
      "Epoch: [26/30]\n",
      "Epoch: [27/30]\n",
      "Epoch: [28/30]\n",
      "Epoch: [29/30]\n",
      "Epoch: [30/30]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, real in enumerate(dataset):\n",
    "        real = real.view(-1, image_dim).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ### Train Discriminator: max log(D(real)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator maximize log(D(G(z)))\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch: [{epoch+1}/{num_epochs}]\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 3, 256, 256)\n",
    "                data = real.reshape(-1, 3, 256, 256)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "                step += 1\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2680cc1-8cd5-4acf-bca8-3350f54a61bf",
   "metadata": {},
   "source": [
    "## 4.Generate example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc8d3a54-1f19-4057-a831-5bd00c97de10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 images generated and saved in generated_images\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming you have 'gen' (generator) already defined and trained\n",
    "generator = gen\n",
    "\n",
    "# Set the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Generate images using random noise\n",
    "num_images = 10  # Number of images to generate\n",
    "z_dim = 64      # Size of the generator's input noise vector\n",
    "\n",
    "# Generate random noise\n",
    "noise = torch.randn(num_images, z_dim).to(device)\n",
    "\n",
    "# Generate fake images\n",
    "fake_images = generator(noise)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = 'generated_images'\n",
    "\n",
    "# Save generated images\n",
    "from torchvision import utils\n",
    "\n",
    "def convert_tensor_to_image(tensor):\n",
    "    \"\"\"Converts a torch tensor to a PIL Image.\"\"\"\n",
    "    image = transforms.ToPILImage()(tensor.reshape(3, 256, 256))\n",
    "    return image\n",
    "\n",
    "# Convert the fake images to PIL Images\n",
    "image_names = [f'generated_image_{i+1}.png' for i in range(num_images)]\n",
    "\n",
    "pil_images = [convert_tensor_to_image(image) for image in fake_images]\n",
    "\n",
    "# Apply the custom transformations\n",
    "preprocessed_images = [transform(image) for image in pil_images]\n",
    "\n",
    "# Save the preprocessed images\n",
    "for image, name in zip(preprocessed_images, image_names):\n",
    "    utils.save_image(image, os.path.join(output_dir, name))\n",
    "\n",
    "print(f'{num_images} images generated and saved in {output_dir}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc64176-d335-47cb-8f51-5a3cc1329268",
   "metadata": {},
   "source": [
    "## 5.Save & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49b1dfad-405d-45c6-b9ff-e81d69dac43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'saved_model/'\n",
    "torch.save(gen.state_dict(), os.path.join(model_dir, 'final_generator.pth'))\n",
    "torch.save(disc.state_dict(), os.path.join(model_dir, 'final_discriminator.pth'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32ef0b17-e78e-4490-a54e-322a3aeb7d20",
   "metadata": {},
   "source": [
    "# Create instances of the model classes\n",
    "loaded_gen = Generator(z_dim, image_dim).to(device)\n",
    "loaded_disc = Discriminator(image_dim).to(device)\n",
    "\n",
    "# Load saved state dictionaries\n",
    "gen_checkpoint = torch.load('saved_model/final_generator.pth')\n",
    "disc_checkpoint = torch.load('saved_model/final_discriminator.pth')\n",
    "\n",
    "loaded_gen.load_state_dict(gen_checkpoint)\n",
    "loaded_disc.load_state_dict(disc_checkpoint)\n",
    "\n",
    "# Set the models to evaluation mode\n",
    "loaded_gen.eval()\n",
    "loaded_disc.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLvenv",
   "language": "python",
   "name": "mlvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
